<html><head><meta charset="utf-8"><title>23 分布式网络爬虫-慕课专栏</title>
			<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
			<meta name="renderer" content="webkit">
			<meta property="qc:admins" content="77103107776157736375">
			<meta property="wb:webmaster" content="c4f857219bfae3cb">
			<meta http-equiv="Access-Control-Allow-Origin" content="*">
			<meta http-equiv="Cache-Control" content="no-transform ">
			<meta http-equiv="Cache-Control" content="no-siteapp">
			<link rel="apple-touch-icon" sizes="76x76" href="https://www.imooc.com/static/img/common/touch-icon-ipad.png">
			<link rel="apple-touch-icon" sizes="120x120" href="https://www.imooc.com/static/img/common/touch-icon-iphone-retina.png">
			<link rel="apple-touch-icon" sizes="152x152" href="https://www.imooc.com/static/img/common/touch-icon-ipad-retina.png">
			<link href="https://moco.imooc.com/captcha/style/captcha.min.css" rel="stylesheet">
			<link rel="stylesheet" href="https://www.imooc.com/static/moco/v1.0/dist/css/moco.min.css?t=201907021539" type="text/css">
			<link rel="stylesheet" href="https://www.imooc.com/static/lib/swiper/swiper-3.4.2.min.css?t=201907021539">
			<link rel="stylesheet" href="https://static.mukewang.com/static/css/??base.css,common/common-less.css?t=2.5,column/zhuanlanChapter-less.css?t=2.5,course/inc/course_tipoff-less.css?t=2.5?v=201907051055" type="text/css">
			<link charset="utf-8" rel="stylesheet" href="https://www.imooc.com/static/lib/ueditor/themes/imooc/css/ueditor.css?v=201907021539"><link rel="stylesheet" href="https://www.imooc.com/static/lib/baiduShare/api/css/share_style0_16.css?v=6aba13f0.css"></head>
			<body><div id="main">

<div class="container clearfix" id="top" style="display: block; width: 1134px;">
    
    <div class="center_con js-center_con l" style="width: 1134px;">
        <div class="article-con">
                            <!-- 买过的阅读 -->
                <div class="map">
                    <a href="/read" target="_blank"><i class="imv2-feather-o"></i></a>
                    <a href="/read/34" target="_blank">从 0 开始学爬虫</a>
                    <a href="" target="_blank">
                        <span>
                            / 5-7 23 分布式网络爬虫
                        </span>
                    </a>
                </div>

            


            <div class="art-title" style="margin-top: 0px;">
                23 分布式网络爬虫
            </div>
            <div class="art-info">
                
                <span>
                    更新时间：2019-06-17 19:43:05
                </span>
            </div>
            <div class="art-top">
                                <img src="https://img.mukewang.com/5d077cc70001bba906400359.jpg" alt="">
                                                <div class="famous-word-box">
                    <img src="https://www.imooc.com/static/img/column/bg-l.png" alt="" class="bg1 bg">
                    <img src="https://www.imooc.com/static/img/column/bg-r.png" alt="" class="bg2 bg">
                    <div class="famous-word">困难只能吓倒懦夫懒汉，而胜利永远属于敢于等科学高峰的人。<p class="author">——茅以升</p></div>
                </div>
                            </div>
            <div class="art-content js-lookimg">
                <div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">当面对像豆瓣读书这样数据量巨大的爬取目标并且由于反爬机制的限制，一次性爬网所需要花费的时间是很长的，面对这种大任务我们可以将爬虫化整为零，将爬取工作分散到多台机器上同时进行，本节将介绍如何采用scrapy-redis 将豆瓣爬虫重构为分布式爬虫，加速爬网的效率。</p>
</div><div class="cl-preview-section"><h2 id="分布式爬网的目的" style="font-size: 30px;">分布式爬网的目的</h2>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">“大巧不攻，以力破法”</p>
</div><div class="cl-preview-section"><blockquote>
<p style="font-size: 20px; line-height: 38px;">制造一个成本极高而无用的天才，不如制造一堆廉价而有用的白痴。</p>
</blockquote>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">当我们应对一些需要进行长期的增量式爬网的场合，而且目标网站的数量规模极为庞大时，只有一台机器去爬取可能完成每次爬取任务所需的时间就会显得非常的慢长。为了增加爬取的效率我们可以：</p>
</div><div class="cl-preview-section"><ul>
<li style="font-size: 20px; line-height: 38px;">将一个很长的任务分切成多个时间点来爬取</li>
<li style="font-size: 20px; line-height: 38px;">将目标爬取数据进行分区、分块的形式分切爬取任务</li>
</ul>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">以上两点是我们在之前的章节中学到的。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">但，如果出现以上两种方法都无法应对的场景时，又或者你想以更短的时间更快的效率完成爬网那么就可以应用爬虫技术中的大招：分布式爬网技术。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">分布式爬网是有条件限制的，开发过程并不复杂但是在部署上对于一般的开发人员而言就有点苛刻。要实现分布式爬网的第一个条件是你要拥有**<u>多台具有独立IP</u>且<u>不在同一内部网络</u>下的可运行爬虫程序的机器**，每一台的机器会部署与运行一套分布式爬虫的可执行副本，如果在同一网络下会导致网络出口的拥挤甚至是堵塞。</p>
</div><div class="cl-preview-section"><blockquote>
<p style="font-size: 20px; line-height: 38px;">分布式爬网可以说是一种富人级别的技术，“以力破法、以本伤人”，拼的是机器的资源。</p>
</blockquote>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">因此，这种技术多用于具有一定规模的商业场景。</p>
</div><div class="cl-preview-section"><h2 id="最简单的分布式爬虫结构" style="font-size: 30px;">最简单的分布式爬虫结构</h2>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">Scrapy并没有提供内置的机制来支持分布式（多服务器）爬取。不过还是有办法进行分布式爬取，这取决于要怎么“分布”了。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">如果有很多Spider，那分布负载最简单的办法就是启动多个Scrapyd，并分配到不同机器上。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">如果有很多Spider，那分布负载最简单的办法就是启动多个Scrapyd，并分配到不同机器上。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">如果想要在多个机器上运行一个单独的Spider，则可以将要爬取的URL进行分块，并发送给Spider。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">首先，准备要爬取的URL列表，并分配到不同文件的URL中：</p>
</div><div class="cl-preview-section"><pre><code>http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">接着在3个不同的Scrapd服务器中启动Spider。Spider会接收一个（Spider）参数part ， 该参数表示要爬取的分块：</p>
</div><div class="cl-preview-section"><pre><code>$ curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
$ curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
$ curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">这种分布式爬虫之间形成的是一个对等网，每个爬虫节点之间没有从属关系，其最大的作用是将爬网任意化，通过控制清单来分配爬网任务。其优点是可以将现有的Scrapy项目直接部署，无须多加改动。但其缺点也是显而易见的——任务列表需要人工分配与更新，可以适用于一些非持久性的轻度增量式爬网场合。</p>
</div><div class="cl-preview-section"><h2 id="scrapy-redis-的架构介绍" style="font-size: 30px;">scrapy-redis 的架构介绍</h2>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;"><a href="https://scrapy-redis.readthedocs.io/en/v0.6.1/readme.html">scrapy-redis</a>是一个基于Redis的Scrapy分布式组件。它利用Redis对用于爬取的请求（Requests）进行存储和调度（Schedule），并对爬取产生的项目（Items）存储以供后续处理使用。scrapy-redis重写了Scrapy中比较关键的代码，将Scrapy变成了一个可以在多个主机上同时运行的分布式爬虫。</p>
</div><div class="cl-preview-section"><h3 id="scrapy-redis分布式原理">scrapy-redis分布式原理</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">假设有4台不同操作系统的计算机：macOS、Ubuntu 14.04、Ubuntu 16.04、CentOS 7.2，任意一台计算机都可以作为Master端或Slaver端。比如：</p>
</div><div class="cl-preview-section"><ul>
<li style="font-size: 20px; line-height: 38px;">Master端（消息服务器） - 使用Ubuntu 16.04，搭建一个Redis数据库，不负责爬取，只负责URL指纹判重、Request的分配，以及数据的存储。</li>
<li style="font-size: 20px; line-height: 38px;">Slaver端（爬虫程序执行端）：使用macOS、Ubuntu 14.04、CentOS 7.2，负责执行爬虫程序，运行过程中提交新的Request给Master。</li>
<li style="font-size: 20px; line-height: 38px;">发布端 — 负责向消息服务器写入爬取任务。这一端可以任何形式的程序，可以是CLI也可以是一个用Flask编写的网站。</li>
</ul>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;"><img src="http://img.mukewang.com/5d0749c900010d2209230763.jpg" alt="图片描述" data-original="http://img.mukewang.com/5d0749c900010d2209230763.jpg" class="" style="cursor: pointer;"></p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">其工作过程如下：</p>
</div><div class="cl-preview-section"><ol>
<li style="font-size: 20px; line-height: 38px;">Master端与Slaver端处于持久运行的服务状态</li>
<li style="font-size: 20px; line-height: 38px;">由发布端向REDIS发起一系列爬取任务（其实就是目标爬取地址）</li>
<li style="font-size: 20px; line-height: 38px;">Slaver通过侦测REDIS的事件会从Master端“拿”任务（Request、URL）进行数据抓取，直至REDIS中的已经没有任务为止。</li>
</ol>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">scrapy-redis就是基于以上的原理实现的，每一个爬虫服务就是一个基于scrapy-redis编写的爬虫项目。Scrapy-Redis是在Scrapy进行扩展的的一种用于订阅Redis事件的爬虫与一系列为此服务的扩展包。我们要实现一个分布式爬虫的SLAVER端只要继承scrapy-redis提供的<code>RedisSpider</code>来实现蜘蛛、在配置文件中对REDIS服务器进行配置就可以了。</p>
</div><div class="cl-preview-section"><h3 id="scrapy-redis的工作机理">scrapy-redis的工作机理</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">如果我们自行编写一个分布式爬虫在多台主机上运行，则需要将爬虫的爬取队列进行共享。也就是说，每台主机都需要访问一个共享的队列，然后爬虫从队列中取一个Request进行爬取。当然这些scrapy-redis都已经帮我们做好了，需要做的是如下操作：</p>
</div><div class="cl-preview-section"><ol>
<li style="font-size: 20px; line-height: 38px;">初始化爬虫，创建一个Redis的客户端，连接Redis。</li>
<li style="font-size: 20px; line-height: 38px;">查看请求队列是否为空，如果是空则等待，当请求的队列不为空，则从请求队列中拿出一个Request。</li>
<li style="font-size: 20px; line-height: 38px;">获得Request后，经过scrapy-redis的调度器(scheduler)调度后，Scrapy引擎(Engine)会将Request取出，交由下载器中间件(Downloader-Middlerwares)向网站发出请求。</li>
<li style="font-size: 20px; line-height: 38px;">当请求发出并返回响应后，下载器中间件会返回给Scrapy引擎(Engine)，Scrapy引擎(Engine)就会将结果返回至用户写的爬虫(<code>RedisSpider</code>)，对结果进行分析处理。产生下一个请求地址又或者直接输出数据项<code>Item</code>。
<ul>
<li style="font-size: 20px; line-height: 38px;">如果得到的是一个Request，则会通过scheduler再次调度，判断Request是否重复，并将Request放入请求队列。</li>
<li style="font-size: 20px; line-height: 38px;">如果已经得到了Item，则Scrapy会将Item交给pipeline处理。</li>
</ul>
</li>
</ol>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">scrapy-redis提供了下面四种组件（components）以实现上述过工作过程：</p>
</div><div class="cl-preview-section"><ul>
<li style="font-size: 20px; line-height: 38px;">Scheduler - 调度器；</li>
<li style="font-size: 20px; line-height: 38px;">Duplication Filter - 去重过滤器；</li>
<li style="font-size: 20px; line-height: 38px;">Item Pipeline - 管道；</li>
<li style="font-size: 20px; line-height: 38px;">Base Spider - 蜘蛛基类。</li>
</ul>
</div><div class="cl-preview-section"><h4 id="scheduler---调度器" style="font-size: 26px;">Scheduler - 调度器</h4>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">Scrapy改造了Python本来的<code>collection.deque</code>（双向队列），形成了自己的<code>Scrapy queue</code>（ <a href="https://github.com/scrapy/queuelib/blob/master/queuelib/queue.py">https://github.com/scrapy/queuelib/blob/master/queuelib/queue.py</a> ），但是Scrapy多个Spider不能共享待爬取队列Scrapy queue， 即Scrapy本身不支持爬虫分布式。scrapy-redis的解决是把这个Scrapy queue换成Redis数据库（也是指Redis队列），在同一个redis-server存放要爬取的Request，以便能让多个Spider去同一个数据库中读取数据。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">Scrapy中跟“待爬队列”直接相关的就是调度器Scheduler，它负责对新的Request进行入列操作（加入Scrapy queue），取出下一个要爬取的Request（从Scrapy queue中取出）等操作。它把待爬队列按照优先级建立了一个字典结构，比如：</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python"><span class="token punctuation">{</span>
    优先级<span class="token number">0</span> <span class="token punctuation">:</span> 队列<span class="token number">0</span>
    优先级<span class="token number">1</span> <span class="token punctuation">:</span> 队列<span class="token number">1</span>
    优先级<span class="token number">2</span> <span class="token punctuation">:</span> 队列<span class="token number">2</span>
<span class="token punctuation">}</span>
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">然后根据Request中的优先级来决定该入哪个队列，出列时则按优先级较小的优先出列。为了管理这个比较高级的队列字典，Scheduler需要提供一系列的方法。但是原来的Scheduler已经无法使用，所以使用Scrapy-redis的scheduler组件。</p>
</div><div class="cl-preview-section"><h4 id="duplication-filter---去重过滤器" style="font-size: 26px;">Duplication Filter - 去重过滤器</h4>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">在Scrapy中用集合实现Request去重功能，Scrapy中把已经发送的Request指纹放入一个集合中，把下一个Request的指纹拿到集合中比对，如果该指纹存在于集合中，说明这个Request发送过了，如果没有则继续操作。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">在scrapy-redis中去重是由Duplication Filter组件实现的，它的实现原理与我们之前介绍过的<code>RedisDupFilter</code>完全相同，相比之下我们更进一步在此基础上还增加了一个基于REDIS的布隆过滤器，所以我们完全可以跳过这个内置的过滤器对象，用我们开发的功能更强大更实用的布隆过程器。</p>
</div><div class="cl-preview-section"><h3 id="数据项管道item-pipeline">数据项管道(Item Pipeline)</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">当引擎将爬取到的Item发给数据项管道(Item Pipeline)处理时，scrapy-redis的内置数据项管道(Item Pipeline)会将爬取到的数据项(Item)存入Redis的Items queue。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">其实，这个做法可圈可点，在实战过程中我觉得这甚是一种鸡肋般的存在，原因非常简单，Redis只能支持20G的内存数据库，要实施分布式爬虫网络的项目数据项目远远会大于20G！这种结构只会让整个分布式爬虫网络因为数据量过大而彻底崩溃！所以并不建议使用，要进行分布式爬取就应该采用分布式数据存储结构来处理数据端，或者将数据直接写入数仓等的方法。</p>
</div><div class="cl-preview-section"><h3 id="scrapy-redis-的蜘蛛">scrapy-redis 的蜘蛛</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">scrapy-redis 不再使用scrapy原有的<code>Spider</code>类，要实现分布式爬虫就必须从scrapy-redis提供的<code>RedisSpider</code>或<code>RedisCrawlSpider</code>中继承。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">scrapy的原生蜘蛛在爬取任务完成后就会直接让整个scrapy进程退出，而 scrapy-redis的蜘蛛却完全不同，它是一个永久运行的蜘蛛或者更准确地说scrapy-redis让scrapy爬取变成了一个服务。即使爬取完成了，它就只会继续等待Redis中的下一个爬取任务而不会消亡。</p>
</div><div class="cl-preview-section"><h2 id="基于-scrapy-redis-改写豆瓣读书爬虫" style="font-size: 30px;">基于 scrapy-redis 改写豆瓣读书爬虫</h2>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">在安装scrapy-redis之前需要先准备个redis服务器或者虚拟机，以提供Master任务队列服务。然后键入以下的指令安装redis工具包与scrapy-redis。</p>
</div><div class="cl-preview-section"><pre><code>$ pip install redis
$ pip install scrapy-redis
</code></pre>
</div><div class="cl-preview-section"><h3 id="配置-scrapy-redis">配置 scrapy-redis</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">首先，我们来起动一个redis的docker:</p>
</div><div class="cl-preview-section"><pre><code>$ mkdir data
$ docker run -p 6379:6379 -v $PWD/data:/data -d redis
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">然后，在<code>settings.py</code>中添加Redis的配置，使用<code>REDIS_URL</code>声明redis服务的访问信息：</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python"><span class="token comment"># Redis设置</span>
REDIS_URL <span class="token operator">=</span> <span class="token string">'redis://127.0.0.1:6379/0'</span>
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">或者，采用另一种配置方式</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python">REDIS_HOST <span class="token operator">=</span> <span class="token string">"127.0.0.1"</span>  <span class="token comment"># REDIS主机</span>
REDIS_PORT <span class="token operator">=</span> <span class="token number">6379</span>               <span class="token comment"># REDIS服务端口</span>
<span class="token comment"># REDIS_PASSWD = "***"            # 访问密码</span>
REDIS_DB <span class="token operator">=</span> <span class="token number">0</span>                    <span class="token comment"># 数据库索引号</span>
</code></pre>
</div><div class="cl-preview-section"><blockquote>
<p style="font-size: 20px; line-height: 38px;">注：<code>REDIS_URL</code>的优先级最高，大于<code>REDIS_*</code>，两种配置方式只要配置一种就好了。</p>
</blockquote>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">接下来配置去重、调度器与数据管道：</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python"><span class="token comment">#使用scrapy-redis中的去重组件</span>
DUPEFILTER_CLASS <span class="token operator">=</span> <span class="token string">"scrapy_redis.dupefilter.RFPDupeFilter"</span>
<span class="token comment"># 使用scrapy-redis中的调度器</span>
SCHEDULER <span class="token operator">=</span> <span class="token string">"scrapy_redis.scheduler.Scheduler"</span>
<span class="token comment"># 允许暂停后,能保存进度</span>
SCHEDULER_PERSIST <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment"># 指定排序爬取地址时使用的队列</span>
<span class="token comment"># 默认的，按优先级排序（Scrapy默认），由sorted set实现的一种非FIFO、LIFO方式</span>
SCHEDULER_QUEUE_CLASS <span class="token operator">=</span> <span class="token string">'scrapy_redis.queue.SpiderPriorityQueue'</span>
<span class="token comment"># 可选的，按先进先出排序（FIFO）</span>
<span class="token comment"># SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderQueue'</span>
<span class="token comment"># 可选的，按后进先出排序（LIFO）</span>
<span class="token comment"># SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderStack'</span>

ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token string">'scrapy_redis.pipelines.RedisPipeline'</span><span class="token punctuation">:</span> <span class="token number">400</span>
<span class="token punctuation">}</span>
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">这里最重要的一点是要配置scrapy-redis的<code>Scheduler</code>，这可以说是scrapy-redis的动作核心。在上述配置中出现了一个去重过滤器，这是scrapy-redis原生搭载的基于Redis的去重过滤器。既然是使用分布式爬虫，那么目标数据一定极为庞大，所以推荐使用在“高效的Redis布隆过滤器”一节中提到的布隆过滤器。最后启用<code>RedisPipeline</code>，将采集后的数据进行存储等后处理。</p>
</div><div class="cl-preview-section"><h3 id="改写分布式蜘蛛">改写分布式蜘蛛</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">scrapy_redis的唯一缺陷就是不能通过中间件技术接入到爬虫系统，所有基于scrapy-redis架构下的蜘蛛都必须继承自 <code>scrapy_redis.spiders.RedisSpider</code> 或<code>scrapy_redis.spiders.RedisCrawlSpider</code>类。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">另外，在蜘蛛中我们将初始URL存放在<code>start_urls</code>类成员中。而<code>在RedisCrawlSpider</code>类中，我们需要初始化的成员是<code>redis_key</code>，这是Redis数据库的一个队列的名。爬虫开始运行的时候，读入<code>settings.py</code>中的Redis配置来访问远程的Redis数据库。如果根据<code>redis_key</code>从数据库中获取初始的URL来爬取爬去，Redis数据库所在的主机就是Master，所有从机（Slaver）都配置好主机的Redis信息，然后运行爬虫，就能不断地从Redis数据库中获取待爬取的URL。</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python"><span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>linkextractors <span class="token keyword">import</span> LinkExtractor
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>loader <span class="token keyword">import</span> ItemLoader
<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>items <span class="token keyword">import</span> BookItem
<span class="token keyword">from</span> scrapy_redis<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> RedisCrawlSpider
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> Rule

<span class="token keyword">class</span> <span class="token class-name">BookSpider</span><span class="token punctuation">(</span>RedisCrawlSpider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"doubanbook"</span>
    rules <span class="token operator">=</span> <span class="token punctuation">(</span>Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'\/tag\/(.*?)'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> follow<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
             Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'\/tag\/(.*?)\?start\='</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                tags<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'link'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attrs<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'href'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> follow<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
             Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'\/subject\/.*'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> follow<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> callback<span class="token operator">=</span><span class="token string">'parse_item'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    redis_key <span class="token operator">=</span> <span class="token string">'spider:start_urls'</span>

    <span class="token keyword">def</span> <span class="token function">parse_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 与原来一至，此处略去</span>
</code></pre>
</div><div class="cl-preview-section"><blockquote>
<p style="font-size: 20px; line-height: 38px;">如果你希望蜘蛛变得更具通用可以将 <code>redis_key</code>的值通过配置文件来指定，如下所示：</p>
<pre><code>REDIS_START_URLS_KEY="spider:start_urls"
</code></pre>
</blockquote>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">分布式蜘蛛的改写仅仅需要将来原继承的基类更改为<code>RedisCrawlSpider</code>，同时将<code>strat_urls</code>去除掉即可。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">此时就可以通过<code>scrapy crawl doubanbook</code>来起动蜘蛛，然后会看到爬虫进程会停着不动，这是正常的因为爬虫现在正处于待命状态，等待从REDIS中发来的爬取任务。</p>
</div><div class="cl-preview-section"><h3 id="编写发布端">编写发布端</h3>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">那谁来产生<code>start_urls</code>的爬网地址呢？存储进Redis<code>spider:start_urls</code>键内的值是由外部程序产生的，是不是很奇怪？确实习惯于单机开发的模式之后一下子很难将思路切换过来，由于蜘蛛代码是被分布在各个节点上的，而且是处于持久运行的状态，只有侦测到Redis中的<code>spider:start_urls</code>有新的URL值的时候才会运行起来，如果在蜘蛛中写入产生URL的逻辑的，那么每个副本节点在运行的时候都会向Redis写入相同的URL这样就乱套了。所以只能将写入<code>start_urls</code>的过程分离到爬虫系统之外，或者是某个脚本代码，又或者可以是某个Web进程。</p>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">在本例中我们就将URL的产生保存在一个<code>pub.py</code>的python脚本中，在任何能访问到Redis的机器上运行该文件就可以启动整个分布式爬虫网络了。</p>
</div><div class="cl-preview-section"><pre class="  language-python"><code class="prism  language-python"><span class="token comment"># coding:utf-8</span>
<span class="token keyword">from</span> redis <span class="token keyword">import</span> Redis

redis <span class="token operator">=</span> Redis<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'127.0.0.1'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">6379</span><span class="token punctuation">,</span> db<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># 写入Redis启动分布式爬虫网络</span>
redis<span class="token punctuation">.</span>lpush<span class="token punctuation">(</span><span class="token string">'spider:start_urls'</span><span class="token punctuation">,</span> <span class="token string">'https://book.douban.com/tag/'</span><span class="token punctuation">)</span>
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">接下来你可以打开一新的终端窗口运行:</p>
</div><div class="cl-preview-section"><pre><code>$ python pub.py
</code></pre>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">你就会看见在待机的爬虫马上就工作起来了！</p>
</div><div class="cl-preview-section"><h2 id="小结" style="font-size: 30px;">小结</h2>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">分布式爬虫的开发就这么简单？对！将前文内容与本节示例归纳一下，可以总结出编写分布式爬虫的几个要点：</p>
</div><div class="cl-preview-section"><ol>
<li style="font-size: 20px; line-height: 38px;">部署Redis服务</li>
<li style="font-size: 20px; line-height: 38px;">定义Item</li>
<li style="font-size: 20px; line-height: 38px;">继承<code>RedisSpider</code>编写Spider
<ul>
<li style="font-size: 20px; line-height: 38px;">定义<code>redis_key</code></li>
</ul>
</li>
<li style="font-size: 20px; line-height: 38px;">在<code>settings.py</code>加入scrapy_redis要求的配置项</li>
<li style="font-size: 20px; line-height: 38px;">将爬虫部署在多台节点上（对于持续迭代的项目建议使用scrapyd部署）</li>
<li style="font-size: 20px; line-height: 38px;">编写生成起始爬取URL的启动脚本</li>
</ol>
</div><div class="cl-preview-section"><p style="font-size: 20px; line-height: 38px;">另外，这个示例里我留下了一些值得我们深思的点，那就作为一个课后的习题让你去发挥吧：</p>
</div><div class="cl-preview-section"><ul>
<li style="font-size: 20px; line-height: 38px;"><code>pub.py</code>只向REDIS发起一个任务(起始URL)，当分布式爬虫部署到网络上之后就只能有一只蜘蛛会运行起来，这明显就失去了分布式的意义，那如何能让这种任务分配得当呢？</li>
<li style="font-size: 20px; line-height: 38px;">配置中将收集到的<code>Item</code>全部通过<code>RedisPipeline</code>写进了REDIS内，既然前文提及不要这样用，那么请思考一下如何将这个存储端换成你所熟悉的一种方式：MongoDB或者PostgreSQL ?</li>
<li style="font-size: 20px; line-height: 38px;">将去重过滤器换成之前章节中介绍的高效的REDIS布隆过滤器对比一下爬取效率？</li>
</ul>
</div><div class="cl-preview-section"><blockquote>
<p style="font-size: 20px; line-height: 38px;">分布式爬虫的源代码我发布在豆瓣爬虫项目的一个scrapy-redis分支上，要切换到该分支，请拉取主分支并进入项目目录后键入以下的指令:</p>
<pre><code>$ git checkout scrapy-redis
</code></pre>
</blockquote>
</div></div>
            </div>
                            <!-- 买过的阅读 -->
                <div class="art-next-prev clearfix">
                                                                        <!-- 已买且开放 或者可以试读 -->
                            <a href="/read/34/article/429">
                                                    <div class="prev l clearfix">
                                <div class="icon l">
                                    <i class="imv2-arrow3_l"></i>
                                </div>
                                <p>
                                    22 反爬之反跟踪与随机代理
                                </p>
                            </div>
                        </a>
                                                                                            <!-- 已买且开放 或者可以试读 -->
                            <a href="/read/34/article/330">
                                                    <div class="next r clearfix">
                                <p>
                                    花瓣网爬虫的分析和设计
                                </p>
                                <div class="icon r">
                                    <i class="imv2-arrow3_r"></i>
                                </div>

                            </div>
                        </a>
                                    </div>
                    </div>
        <div class="comments-con js-comments-con" id="coments_con">
        </div>



    </div>
    
    
    

</div>
 
<!-- 专栏介绍页专栏评价 -->

<!-- 专栏介绍页底部三条评价 -->

<!-- 专栏阅读页弹层目录和介绍页页面目录 -->

<!-- 专栏阅读页发布回复 -->

<!-- 专栏阅读页发布评论 -->

<!-- 专栏阅读页底部评论 -->

<!-- 专栏阅读 单个 评论 -->

<!-- 新增回复和展开三条以外回复 -->

<!-- 立即订阅的弹窗 -->












</div></body></html>